# -*- coding: utf-8 -*-
"""punc tokenizer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ElwfpmG8A3GgpDrAWjw3IbkQ1rBhQlKx
"""

from nltk.tokenize import WordPunctTokenizer 
  
# WordPunctTokenizer â€“ It seperates the punctuation from the words

tokenizer = WordPunctTokenizer() 
tokenizer.tokenize("Let's see how it's working.")

#Using Regular Expression


from nltk.tokenize import RegexpTokenizer 
  
tokenizer = RegexpTokenizer("[\w']+") 
text = "Let's see how it's working."
tokenizer.tokenize(text)

